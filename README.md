# ðŸ“˜ PDF-Based Retrieval-Augmented Generation (RAG)

A fully **local Retrieval-Augmented Generation (RAG)** system that enables natural-language querying of a PDF using **Ollama**, **FAISS**, and **LangChain**.

This project demonstrates how to:
- Ingest and chunk structured PDFs
- Embed text locally using Ollama
- Store and search embeddings with FAISS
- Generate grounded answers using an LLM

---

## ðŸš€ Features

- ðŸ“„ PDF ingestion via **PyMuPDF**
- ðŸ§  Semantic search using **FAISS**
- ðŸ”— Retrieval-Augmented Generation (RAG)
- ðŸ¤– Local embeddings + LLMs via **Ollama**
- ðŸ›‘ Hallucination control via prompt grounding
- ðŸ’» Runs fully **offline** after setup

---

## ðŸ—ï¸ Project Architecture
User Question
â†“
Embedding (nomic-embed-text)
â†“
FAISS Vector Search
â†“
Relevant PDF Chunks
â†“
LLM (llama3.1)
â†“
Grounded Answer


---

## ðŸ“‚ Project Structure
rag-pdf-qa/
â”œâ”€â”€ ingest.py # PDF ingestion + FAISS index creation
â”œâ”€â”€ query.py # Query-time RAG pipeline
â”œâ”€â”€ data/ # PDF input (ignored in git)
â””â”€â”€ index/ # FAISS index (ignored in git)


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/hasham-tdl/rag-pdf-qa.git
cd rag-pdf-qa

### 2ï¸âƒ£ Create and activate a virtual environment
python -m venv venv
venv\Scripts\activate

###3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

###4ï¸âƒ£ Install & run Ollama
Download Ollama from https://ollama.com

Pull the required models:
- ollama pull nomic-embed-text
- ollama pull llama3.1

###5ï¸âƒ£ Add your PDF
data/book.pdf

###6ï¸âƒ£ Build the FAISS index
python ingest.py

###7ï¸âƒ£ Query the document
python query.py

ðŸ§  Example Query
Ask a question:
> What is OAuth authentication?

OAuth is an authorization framework that...

ðŸ§ª Models Used
Purpose	Model
Embeddings	nomic-embed-text
LLM	llama3.1

ðŸ“Œ Notes

- The system answers only using retrieved PDF content
- If an answer is not found, it responds accordingly
- Chunking is optimized for QA-style documents

